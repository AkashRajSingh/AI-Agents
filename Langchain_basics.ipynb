{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPennOR8T3iP10OiY6F8ovC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AkashRajSingh/AI-Agents/blob/main/Langchain_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is Langchain?\n",
        "LangChain is an open source framework that helps in building LLM based applications. It\n",
        "provides modular components and end-to-end tools that help developers build complex AI\n",
        "applications, such as chatbots, question-answering systems, retrieval-augmented generation\n",
        "(RAG), autonomous agents, and more.\n",
        "1. Supports all the major LLMs\n",
        "2. Simplifies developing LLM based applications (through chains)\n",
        "3. Integrations available for all major tools\n",
        "4. Open source/Free/Actively developed\n",
        "5. Supports all major GenAI use cases\n",
        "\n",
        "### Benefits of Langchain:\n",
        "- Concept of chains\n",
        "- Model Agnostic Development (has done standardization across all models)\n",
        "- Complete ecosystem\n",
        "- Memory and state handling\n",
        "\n",
        "### What can you build using Langchain?\n",
        "\n",
        "- Conversational Chatbots\n",
        "- AI Knowledge Assistants\n",
        "- AI Agents\n",
        "- Workflow Automation\n",
        "- Summarization/Research Helpers\n",
        "- and much more\n",
        "\n",
        "## Langchain Components\n",
        "1. Model\n",
        "2. Prompts\n",
        "3. Chains\n",
        "4. Memory\n",
        "5. Indexes\n",
        "6. Agents\n",
        "\n",
        "Here is a brief description about all these-\n",
        "\n",
        "### 1. Models\n",
        "In LangChain, “models” are the core interfaces through which you interact with AI models. \\\n",
        "Example- \"gpt-4o\", \"claude-3.5-sonnet\" etc. \\\n",
        "\n",
        "Langchain has essentially 2 main types of models-\n",
        "- Language Model: Text input and text output; used for language generation etc.\n",
        "- Embedding models: Text input and vector output; used for semantic search mainly.\n",
        "\n",
        "\n",
        "These models are essentially accessed via APIs (unless you're working with local models). In simple terms, the LLM is on the provider company's server and people can send their requests and get returns from those LLMs via these APIs.\n",
        "\n",
        "\n",
        "### 2.  Prompts\n",
        "The inputs provided to the LLMs are known as Prompts.\n",
        "Are of different types:\n",
        "- Static or Non changing. \\\n",
        "Example- 'Summarize Langchain, imagining that you are an AI Engineer'\n",
        "- Dynamic and reusable prompts. \\\n",
        "Example: 'Summarize {topic}, imagining that you are {role}'.\n",
        "- Role Based Prompting. \\\n",
        "Example: (\"system\", \"You are an {role}), (\"user\", \"Summarize {topic}\")\n",
        "- Few shot prompting- Here you give a few examples to the LLM, to help it understand the task. This is based on an **emergent propert of LLMs**.\n",
        "\n",
        "### 3. Chains\n",
        "Also known as pipelines, basically they are just connectors that connect one component to another.\n",
        "### 4. Memory\n",
        "LLM API calls are stateless; hence to keep track of the conversation and context, memory is needed.\n",
        "A few types are:\n",
        "- ConversationBufferMemory: Stores a transcript of recent messages. Great for\n",
        "short chats but can grow large quickly.\n",
        "- ConversationBufferWindowMemory: Only keeps the last N interactions to avoid\n",
        "excessive token usage.\n",
        "\n",
        "- Summarizer-Based Memory: Periodically summarizes older chat segments to keep\n",
        "a condensed memory footprint.\n",
        "\n",
        "- Custom Memory: For advanced use cases, you can store specialized state (e.g.,\n",
        "the user’s preferences or key facts about them) in a custom memory class.\n",
        "\n",
        "### 5. Indexes\n",
        "Indexes connect your application to external knowledge—such as PDFs,\n",
        "websites or databases\n",
        "\n",
        "### 6. Agents\n",
        "The final autonomous blocks that are capable of doing work independently (as specified by humans).\n",
        "____\n",
        "We'll cover these one by one.\n"
      ],
      "metadata": {
        "id": "I27YBMeHbj2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Models\n",
        "The Model Component in LangChain is a crucial part of the framework, designed to facilitate\n",
        "interactions with various language models and embedding models.\n",
        "It abstracts the complexity of working directly with different LLMs, chat models, and\n",
        "embedding models, providing a uniform interface to communicate with them. This makes it\n",
        "easier to build applications that rely on AI-generated text, text embeddings for similarity\n",
        "search, and retrieval-augmented generation (RAG).\n",
        "- Models in Langchain can be bifurcated as mentioned below:\n",
        "\n",
        "                        Langchain Model\n",
        "                             /       \\\n",
        "              Language Model          Embedding Model\n",
        "              /          \\\n",
        "          LLMs            Chat Model\n",
        "                           /       \\\n",
        "          Open Source Models         Closed Source/ Propreity Models\n",
        "\n",
        "### Language Models\n",
        "Language Models are AI systems designed to process, generate, and understand natural language text.\n",
        "- LLMs - General-purpose models that is used for raw text generation. They take a string(or plain text) as input and returns a string( plain text). These are traditionally older models and are not used much now.\n",
        "- Chat Models - Language models that are specialized for conversational tasks. They take a sequence of messages as inputs and return chat messages as outputs (as opposed to using plain text). These are traditionally newer models and used more in comparison to the LLMs. \\\n",
        "\n",
        "Here is a detailed comparison table:\n",
        "\n",
        "| **Feature**          | **LLMs (Base Models)**                                                         | **Chat Models (Instruction-Tuned)**                                          |\n",
        "| -------------------- | ------------------------------------------------------------------------------ | ---------------------------------------------------------------------------- |\n",
        "| **Purpose**          | Designed for free-form text generation                                         | Optimized for multi-turn, structured conversations                           |\n",
        "| **Training Data**    | Trained on general corpora like books and articles                             | Fine-tuned on chat datasets (dialogues, user-assistant interactions)         |\n",
        "| **Memory & Context** | No built-in memory or structured context                                       | Supports structured conversation history                                     |\n",
        "| **Role Awareness**   | No concept of “user” and “assistant” roles                                     | Understands roles like \"system\", \"user\", and \"assistant\"                     |\n",
        "| **Example Models**   | GPT-3, LLaMA 2-7B, Mistral-7B, OPT-1.3B                                        | GPT-4, GPT-3.5 Turbo, LLaMA-2-Chat, Mistral-Instruct, Claude                 |\n",
        "| **Use Cases**        | Text generation, summarization, translation, creative writing, code generation | Conversational AI, chatbots, virtual assistants, customer support, AI tutors |\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JrErK1ATdfgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Models\n",
        "# Not much used now\n",
        "\n",
        "from langchain_openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = OpenAI(model='gpt-3.5-turbo-instruct')\n",
        "\n",
        "result = llm.invoke(\"What is the capital of India\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "YYVqGMD7bk2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting with Chat Models, understand the 2 types of the chat Models:\n",
        "\n",
        "### 1. Closed-Source Language Models\n",
        "\n",
        "Closed-source models are proprietary LLMs developed and hosted by companies like OpenAI, Anthropic, and Google. These models are accessed via **API calls** (you send a request to their servers and receive a generated response).\n",
        "\n",
        "#### Advantages\n",
        "\n",
        "* State-of-the-art performance (e.g., GPT-4, Claude, Gemini)\n",
        "* No setup or infrastructure required\n",
        "* Fine-tuned using **RLHF** (Reinforcement Learning from Human Feedback), which improves instruction-following and response quality\n",
        "\n",
        "#### Limitations\n",
        "\n",
        "* Usage is billed per token or request (pay-as-you-go pricing)\n",
        "* No access to model weights or customization\n",
        "* Data privacy concerns — all input passes through the provider’s servers\n",
        "* Deployment is locked to the provider’s API; cannot run offline or on your own infrastructure\n",
        "\n",
        "#### Examples of Closed Models\n",
        "\n",
        "* GPT-4 / GPT-3.5 by OpenAI\n",
        "* Claude by Anthropic\n",
        "* Gemini by Google\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "KxwoBycb4Cu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Models\n",
        "\n",
        "# Closed Source / Propreity Models\n",
        "\n",
        "#-----------------------------------------------------------------------\n",
        "# OpenAI\n",
        "#-----------------------------------------------------------------------\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatOpenAI(model='gpt-4', temperature=1.5, max_completion_tokens=10)\n",
        "\n",
        "result = model.invoke(\"Write a 5 line poem on cricket\")\n",
        "\n",
        "print(result.content)\n",
        "\n",
        "# Here, Temperature controls the randomness of the model's output;\n",
        "# it ranges from 0 (deterministic) to 1 (highly creative and random).\n",
        "\n",
        "# max_completion_tokens sets the maximum number of tokens the model can generate\n",
        "# in a single response.\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "# Anthropic\n",
        "#------------------------------------------------------------------------------\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatAnthropic(model='claude-3-5-sonnet-20241022')\n",
        "\n",
        "result = model.invoke('What is the capital of India')\n",
        "\n",
        "print(result.content)\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# Google / Gemini\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "from langchain_google import ChatGoogle\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatGoogle(model='google/palm-2-chat-bison')\n",
        "\n",
        "result = model.invoke('What is the capital of India')\n",
        "\n",
        "print(result.content)\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# Hugging Face\n",
        "#--------------------------------------------------------------\n",
        "\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    task=\"text-generation\"\n",
        ")\n",
        "\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "result = model.invoke(\"What is the capital of India\")\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "6-g9FM2U1-Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Open Source Language Models\n",
        "\n",
        "Open-source language models are **freely available AI models** that can be **downloaded**, **modified**, **fine-tuned**, and **deployed** without restrictions from a central provider.\n",
        "\n",
        "> Unlike closed-source models such as **OpenAI’s GPT**, **Anthropic’s Claude**, or **Google’s Gemini**, open-source models give developers full control and ensure better data privacy.\n",
        "\n",
        "---\n",
        "\n",
        "#### Open Source vs Closed Source: Comparison Table\n",
        "\n",
        "| **Feature**       | **Open-Source Models**                                 | **Closed-Source Models**                        |\n",
        "| ----------------- | ------------------------------------------------------ | ----------------------------------------------- |\n",
        "| **Cost**          | Free to use (no API costs)                             | Paid API usage (e.g., OpenAI charges per token) |\n",
        "| **Control**       | Full control — modify, fine-tune, and deploy anywhere  | Locked to provider's infrastructure             |\n",
        "| **Data Privacy**  | Runs locally (no data sent to external servers)        | Sends queries to vendor’s servers               |\n",
        "| **Customization** | Can fine-tune on specific datasets                     | No access to fine-tuning in most cases          |\n",
        "| **Deployment**    | Can be deployed on **on-premise servers** or **cloud** | Must use vendor’s API                           |\n",
        "\n",
        "---\n",
        "\n",
        "#### Why Choose Open Source?\n",
        "\n",
        "* Closed-source models are costly and tied to third-party infrastructure.\n",
        "* Your data is exposed to external servers.\n",
        "Open-source models solve this with **free**, **customizable**, and **self-hostable** alternatives.\n",
        "\n",
        "---\n",
        "\n",
        "#### Some Famous Open Source Models\n",
        "\n",
        "| **Model**              | **Developer** | **Parameters** | **Best Use Case**                                |\n",
        "| ---------------------- | ------------- | -------------- | ------------------------------------------------ |\n",
        "| **LLaMA-2-7B/13B/70B** | Meta AI       | 7B - 70B       | General-purpose text generation                  |\n",
        "| **Mixtral-8x7B**       | Mistral AI    | 8x7B (MoE)     | Efficient & fast responses                       |\n",
        "| **Mistral-7B**         | Mistral AI    | 7B             | Best small-scale model (outperforms LLaMA-2-13B) |\n",
        "| **Falcon-7B/40B**      | TII UAE       | 7B - 40B       | High-speed inference                             |\n",
        "| **BLOOM-176B**         | BigScience    | 176B           | Multilingual text generation                     |\n",
        "| **GPT-J-6B**           | EleutherAI    | 6B             | Lightweight and efficient                        |\n",
        "| **GPT-NeoX-20B**       | EleutherAI    | 20B            | Large-scale applications                         |\n",
        "| **StableLM**           | Stability AI  | 3B - 7B        | Compact models for chatbots                      |\n",
        "\n",
        "---\n",
        "\n",
        "#### Where to Find Open Source LLMs?\n",
        "\n",
        "You can explore and use open-source models via:\n",
        "\n",
        "* [**Hugging Face**](https://huggingface.co/models) — The largest repository of open-source LLMs.\n",
        "\n",
        "---\n",
        "\n",
        "### Ways to Use Open-Source Models\n",
        "\n",
        "```\n",
        "                 Open-Source Models\n",
        "                    /        \\\n",
        "                   /          \\\n",
        "   Using HF Inference API   Running Locally\n",
        "         (Free tier)         (Full control)\n",
        "```\n",
        "\n",
        "1. **Using Hugging Face Inference API**\n",
        "\n",
        "   * No setup needed\n",
        "   * Often comes with free tier access\n",
        "   * Requires an API key\n",
        "\n",
        "2. **Running Locally**\n",
        "\n",
        "   * Maximum privacy and control\n",
        "   * Can be used offline\n",
        "   * Requires GPU and installation of dependencies\n",
        "\n",
        "---\n",
        "\n",
        "#### Disadvantages of Open-Source Models\n",
        "\n",
        "| **Disadvantage**                 | **Details**                                                                     |\n",
        "| -------------------------------- | ------------------------------------------------------------------------------- |\n",
        "| **High Hardware Requirements**   | Running large models (e.g., LLaMA-2-70B) requires expensive GPUs                |\n",
        "| **Setup Complexity**             | Needs installation of dependencies like PyTorch, CUDA, transformers             |\n",
        "| **Lack of RLHF**                 | Most models lack fine-tuning with human feedback (weaker instruction-following) |\n",
        "| **Limited Multimodal Abilities** | Most don’t support images, audio, or video (unlike GPT-4V, Gemini, etc.)        |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1gI05qtB5djg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------\n",
        "# HuggingFace i.e Local Models\n",
        "# (slightly different)\n",
        "#-----------------------------------------------------\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
        "import os\n",
        "\n",
        "os.environ['HF_HOME'] = 'D:/huggingface_cache'\n",
        "\n",
        "llm = HuggingFacePipeline.from_model_id(\n",
        "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
        "    task='text-generation',\n",
        "    pipeline_kwargs=dict(\n",
        "        temperature=0.5,\n",
        "        max_new_tokens=100\n",
        "    )\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "result = model.invoke(\"What is the capital of India\")\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "gF0QVkps22tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Embedding Models\n",
        "\n",
        "An **embedding model** converts text into high-dimensional numerical vectors that capture the semantic meaning of the text. Similar texts have similar embeddings, even if they use different words.\n",
        "\n",
        "> Example: \"I love dogs\" and \"Dogs are great\" will have closely aligned vector representations.\n",
        "\n",
        "Popular models: `OpenAIEmbedding`, `HuggingFaceEmbeddings`, `SentenceTransformers`\n",
        "\n",
        "---\n",
        "\n",
        "### Semantic Search (just read it once for now, we'll cover it later again)\n",
        "\n",
        "**Semantic search** uses embeddings to find the most relevant documents based on meaning, not just exact keywords.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "1. All documents are converted into embeddings and stored in a vector database (e.g., FAISS, Chroma).\n",
        "2. A user query is embedded.\n",
        "3. The system retrieves documents with vectors closest to the query embedding.\n",
        "\n",
        "> Useful for tasks like document retrieval, FAQ bots, and context-aware Q\\&A systems.\n"
      ],
      "metadata": {
        "id": "fvN6dwI6_A98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---------------\n",
        "# Open AI (single query)\n",
        "# ---------------\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)\n",
        "\n",
        "result = embedding.embed_query(\"Delhi is the capital of India\")\n",
        "\n",
        "print(str(result))\n",
        "\n",
        "#---------------\n",
        "# Open AI (as docs)\n",
        "# ---------------\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=32)\n",
        "\n",
        "documents = [\n",
        "    \"Delhi is the capital of India\",\n",
        "    \"Kolkata is the capital of West Bengal\",\n",
        "    \"Paris is the capital of France\"\n",
        "]\n",
        "\n",
        "result = embedding.embed_documents(documents)\n",
        "\n",
        "print(str(result))\n",
        "\n",
        "#---------------\n",
        "# HuggingFace (as docs)\n",
        "# ---------------\n",
        "\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "documents = [\n",
        "    \"Delhi is the capital of India\",\n",
        "    \"Kolkata is the capital of West Bengal\",\n",
        "    \"Paris is the capital of France\"\n",
        "]\n",
        "\n",
        "vector = embedding.embed_documents(documents)\n",
        "\n",
        "print(str(vector))\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "# Practical Application- Semantic Search\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from dotenv import load_dotenv\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "embedding = OpenAIEmbeddings(model='text-embedding-3-large', dimensions=300)\n",
        "\n",
        "documents = [\n",
        "    \"Virat Kohli is an Indian cricketer known for his aggressive batting and leadership.\",\n",
        "    \"MS Dhoni is a former Indian captain famous for his calm demeanor and finishing skills.\",\n",
        "    \"Sachin Tendulkar, also known as the 'God of Cricket', holds many batting records.\",\n",
        "    \"Rohit Sharma is known for his elegant batting and record-breaking double centuries.\",\n",
        "    \"Jasprit Bumrah is an Indian fast bowler known for his unorthodox action and yorkers.\"\n",
        "]\n",
        "\n",
        "query = 'tell me about bumrah'\n",
        "\n",
        "doc_embeddings = embedding.embed_documents(documents)\n",
        "query_embedding = embedding.embed_query(query)\n",
        "\n",
        "scores = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
        "\n",
        "index, score = sorted(list(enumerate(scores)),key=lambda x:x[1])[-1]\n",
        "\n",
        "print(query)\n",
        "print(documents[index])\n",
        "print(\"similarity score is:\", score)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7oA4VcJE_0Fv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prompts in LangChain\n",
        "\n",
        "Prompt engineering is a key part of building LLM-based applications. In LangChain, prompts can be **static** or **dynamic**, and structured in a way that supports both **single-turn queries** and **multi-turn conversations**.\n",
        "\n",
        "---\n",
        "                                 Model (invoke)\n",
        "                                /              \\\n",
        "                               /               \\\n",
        "                Single Message                  List of Messages\n",
        "        (single-turn standalone query)       (multi-turn conversation)\n",
        "                /           \\                    /              \\\n",
        "               /             \\                  /                \\\n",
        "     Static Message   Dynamic Message     Static Message    Dynamic Message\n",
        "                    (PromptTemplate)    (SystemMessage,     (ChatPromptTemplate)\n",
        "                                           HumanMessage,\n",
        "                                           AIMessage)\n",
        "\n",
        "---\n",
        "        \n",
        "\n",
        "### Static vs Dynamic Prompts\n",
        "\n",
        "* **Static Prompt**: A hardcoded string used directly as input to the model. These are suitable for fixed queries with no need for variability.\n",
        "  - Example: \"Translate the following sentence to French: 'Hello, how are you?'\"\n",
        "  \n",
        "\n",
        "* **Dynamic Prompt**: Built at runtime by injecting variables into a template. This is useful when prompts need to change based on user input or application context.\n",
        "\n",
        "---\n",
        "\n",
        "### PromptTemplate in LangChain\n",
        "\n",
        "A PromptTemplate is a structured and reusable way to create prompts by using placeholders that can be dynamically filled in at runtime.\n",
        "\n",
        "##### Example:\n",
        "\n",
        "- \"Translate the following sentence to {language}: '{sentence}'\"\n",
        "\n",
        "\n",
        "#### Why use PromptTemplate instead of f-strings?\n",
        "\n",
        "1. **Validation**: Ensures all variables are provided before formatting, preventing runtime errors.\n",
        "2. **Reusability**: You can reuse the same template across different workflows and models.\n",
        "3. **LangChain Ecosystem Compatibility**: PromptTemplates integrate seamlessly with chains, agents, memory, and tools in LangChain.\n",
        "\n",
        "---\n",
        "\n",
        "### Message-Based Prompting\n",
        "\n",
        "LangChain supports both **single message** (single-turn query) and **list of messages** (multi-turn conversations) for chat models.\n",
        "\n",
        "#### Types of Messages\n",
        "\n",
        "LangChain defines structured message types to model conversation roles:\n",
        "\n",
        "* SystemMessage – Sets behavior or persona of the AI\n",
        "* HumanMessage – User input\n",
        "* AIMessage – AI's response\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### ChatPromptTemplate & MessagesPlaceholder\n",
        "\n",
        "In multi-turn conversations, you may want to dynamically insert prior chat history into the prompt. For this, LangChain offers:\n",
        "\n",
        "#### MessagesPlaceholder\n",
        "\n",
        "A special placeholder used inside ChatPromptTemplate that can be filled at runtime with a list of past messages (chat history).\n",
        "\n",
        "#### Example:\n",
        "\n",
        "```python\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessage(content=\"You're a travel planner.\"),\n",
        "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "    HumanMessage(content=\"Suggest a 3-day itinerary for Paris.\")\n",
        "])\n",
        "```\n",
        "\n",
        "At runtime, chat_history can be populated with a list of messages to maintain conversational context.\n",
        "\n",
        "---\n",
        "\n",
        "By using PromptTemplate and MessagesPlaceholder, LangChain enables you to build more dynamic, reusable, and context-aware LLM workflows - essential for chatbots, agents, and real-world applications.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "LjsbzdYs8gX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# UI for the streamlit app for showing a Dynamic prompt example\n",
        "\n",
        "# PromptTemplate class for dynamic messages (single messages)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "import streamlit as st\n",
        "from langchain_core.prompts import PromptTemplate,load_prompt\n",
        "\n",
        "load_dotenv()\n",
        "model = ChatOpenAI()\n",
        "\n",
        "st.header('Reasearch Tool')\n",
        "\n",
        "paper_input = st.selectbox( \"Select Research Paper Name\", [\"Attention Is All You Need\", \"BERT: Pre-training of Deep Bidirectional Transformers\", \"GPT-3: Language Models are Few-Shot Learners\", \"Diffusion Models Beat GANs on Image Synthesis\"] )\n",
        "\n",
        "style_input = st.selectbox( \"Select Explanation Style\", [\"Beginner-Friendly\", \"Technical\", \"Code-Oriented\", \"Mathematical\"] )\n",
        "\n",
        "length_input = st.selectbox( \"Select Explanation Length\", [\"Short (1-2 paragraphs)\", \"Medium (3-5 paragraphs)\", \"Long (detailed explanation)\"] )\n",
        "\n",
        "template = { #Can be stored as a json file too. example: template.json, to make the code reusable\n",
        "    \"name\": null,\n",
        "    \"input_variables\": [\n",
        "        \"length_input\",\n",
        "        \"paper_input\",\n",
        "        \"style_input\"\n",
        "    ],\n",
        "    \"optional_variables\": [],\n",
        "    \"output_parser\": null,\n",
        "    \"partial_variables\": {},\n",
        "    \"metadata\": null,\n",
        "    \"tags\": null,\n",
        "    \"template\": \"\\nPlease summarize the research paper titled \\\"{paper_input}\\\" with the following specifications:\\nExplanation Style: {style_input}  \\nExplanation Length: {length_input}  \\n1. Mathematical Details:  \\n   - Include relevant mathematical equations if present in the paper.  \\n   - Explain the mathematical concepts using simple, intuitive code snippets where applicable.  \\n2. Analogies:  \\n   - Use relatable analogies to simplify complex ideas.  \\nIf certain information is not available in the paper, respond with: \\\"Insufficient information available\\\" instead of guessing.  \\nEnsure the summary is clear, accurate, and aligned with the provided style and length.\\n\",\n",
        "    \"template_format\": \"f-string\",\n",
        "    \"validate_template\": true,   #benefit of PromptTemplate over f-string\n",
        "    \"_type\": \"prompt\"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "if st.button('Summarize'):\n",
        "    chain = template | model\n",
        "    result = chain.invoke({\n",
        "        'paper_input':paper_input,\n",
        "        'style_input':style_input,\n",
        "        'length_input':length_input\n",
        "    })\n",
        "    st.write(result.content)"
      ],
      "metadata": {
        "id": "i58xIjWI-IuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# understanding different types of Messages\n",
        "# list of messages-static\n",
        "\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "messages=[\n",
        "    SystemMessage(content='You are a helpful assistant'),\n",
        "    HumanMessage(content='Tell me about LangChain')\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "\n",
        "messages.append(AIMessage(content=result.content))\n",
        "\n",
        "print(messages)\n"
      ],
      "metadata": {
        "id": "SO64gJg_msZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat Template/ Messages\n",
        "# Making a chatbot\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "chat_history = [\n",
        "    SystemMessage(content='You are a helpful AI assistant') #[For keeping the context/ memory]\n",
        "]\n",
        "\n",
        "while True:\n",
        "    user_input = input('You: ')\n",
        "    chat_history.append(HumanMessage(content=user_input))\n",
        "    if user_input == 'exit':\n",
        "        break\n",
        "    result = model.invoke(chat_history)\n",
        "    chat_history.append(AIMessage(content=result.content))\n",
        "    print(\"AI: \",result.content)\n",
        "\n",
        "print(chat_history)"
      ],
      "metadata": {
        "id": "ScjLZ4S5hwoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chat prompt template class (for dynamic, list of messages)\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "chat_template = ChatPromptTemplate([\n",
        "    ('system', 'You are a helpful {domain} expert'),\n",
        "    ('human', 'Explain in simple terms, what is {topic}')\n",
        "])\n",
        "\n",
        "prompt = chat_template.invoke({'domain':'cricket','topic':'Dusra'})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "tlg0c8yMmT77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Message Placeholder - to store previous chat history. example- complain tickets raised on Airtel\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "# chat template\n",
        "chat_template = ChatPromptTemplate([\n",
        "    ('system','You are a helpful customer support agent'),\n",
        "    MessagesPlaceholder(variable_name='chat_history'),\n",
        "    ('human','{query}')\n",
        "])\n",
        "\n",
        "chat_history = []\n",
        "# load chat history\n",
        "with open('chat_history.txt') as f:\n",
        "    chat_history.extend(f.readlines())\n",
        "\n",
        "print(chat_history)\n",
        "\n",
        "# create prompt\n",
        "prompt = chat_template.invoke({'chat_history':chat_history, 'query':'Where is my refund'})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "3-Rsq_nFpRrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dealing with outputs we get from LLMs\n",
        "\n",
        "##Structured Output\n",
        "**Structured Output in LangChain**\n",
        "Structured output refers to formatting a language model’s response in a well-defined schema (like JSON) instead of plain text. This makes it easier to parse and use programmatically — especially for downstream tasks like storing, querying, or integrating with other systems. LangChain supports enforcing such formats to turn free-form answers into structured data.\n",
        "\n",
        "example: \\\n",
        "\n",
        "- Query = {\"prompt\": \"Plan a 1-day Paris itinerary\"}  \n",
        "   |\n",
        "-LLM Output = \"Morning: Visit the Eiffel Tower. Afternoon: Walk through the Louvre. Evening: Dinner by the Seine.\"  \n",
        "  |\n",
        "- Structured Output = [\n",
        "  {\"time\": \"Morning\", \"activity\": \"Visit the Eiffel Tower\"},\n",
        "  {\"time\": \"Afternoon\", \"activity\": \"Walk through the Louvre\"},\n",
        "  {\"time\": \"Evening\", \"activity\": \"Dinner by the Seine\"}\n",
        "]\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **Ways to Get Structured Outputs**\n",
        "\n",
        "To make LLM outputs predictable and machine-readable (e.g., JSON, dicts), we can use structured output techniques.\n",
        "There are **two main ways** to get structured outputs from an LLM:\n",
        "\n",
        "```\n",
        "                             Ways to Get Structured Outputs\n",
        "                             _______________________________\n",
        "                            /                               \\\n",
        "         With Structured Output Parsers               Output Parsers (post-process)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **1. With Structured Output Parsers**\n",
        "\n",
        "These guide the LLM **during generation**, enforcing structure as part of the prompt + parsing pipeline.\n",
        "This method is more **reliable and robust** than post-processing free text.\n",
        "\n",
        "We typically define the expected structure using one of these:\n",
        "\n",
        "#### **Types of Structured Parsers**\n",
        "\n",
        "```\n",
        "Structured Output Parsers\n",
        "├── TypedDict\n",
        "├── Pydantic\n",
        "└── JSON Schema\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **What Are These & When to Use Them**\n",
        "\n",
        "#### **TypedDict**\n",
        "\n",
        "* From Python’s `typing` module.\n",
        "* Lightweight way to define the expected dictionary structure.\n",
        "* Good for simple use cases where no validation is required.\n",
        "* **Use when:** You need quick type hints and field names only.\n",
        "\n",
        "#### **Pydantic**\n",
        "\n",
        "* A powerful data validation library (`BaseModel`).\n",
        "* Supports type checking, defaults, optional fields, and complex validations.\n",
        "* **Use when:** You need strong validation and automatic error handling.\n",
        "\n",
        "#### **JSON Schema**\n",
        "\n",
        "* Language-agnostic way to describe data formats in JSON.\n",
        "* More expressive and interoperable across systems.\n",
        "* **Use when:** You need strict structure and want LLMs to output a JSON format usable across platforms.\n",
        "\n",
        "---\n",
        "\n",
        "Additionallly:\n",
        "\n",
        "### **Structured Output Methods (During Inference)**\n",
        "\n",
        "Structured output can be enforced **at generation time** using two main techniques:\n",
        "\n",
        "```\n",
        "           Structured Output (at generation)\n",
        "           _________________________________\n",
        "          /                                 \\\n",
        "     JSON-based Generation             Function Calling\n",
        "```\n",
        "\n",
        "#### **a. JSON-based Generation**\n",
        "\n",
        "* The model is prompted or guided to output valid JSON directly.\n",
        "* Works best when paired with a JSON parser like `Pydantic`, `TypedDict`, or `JSON Schema`.\n",
        "* Relies on model **understanding formatting** well.\n",
        "* Supported by most LLM APIs (OpenAI, Claude, Gemini, etc.)\n",
        "* **Use when:** You just want the model to return a clean JSON output that you can parse directly.\n",
        "\n",
        "#### **b. Function Calling**\n",
        "\n",
        "* Instead of raw text or JSON, the model is given **tools/functions it can \"call\"** with structured arguments.\n",
        "* The LLM internally selects the tool and fills in the arguments in structured format.\n",
        "*  **OpenAI** introduced this (GPT-4), now standard.\n",
        "*  **Claude** supports it (Tool Use API).\n",
        "*  **Gemini** supports function calling via tools in Google AI Studio.\n",
        "* **Use when:** You want to integrate models with **external tools or code**, like search, database access, calculators, or custom functions.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "p4uq-S7Hqc_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#typedict\n",
        "\n",
        "from typing import TypedDict\n",
        "\n",
        "class Person(TypedDict):\n",
        "\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "new_person: Person = {'name':'nitish', 'age':'35'}\n",
        "\n",
        "print(new_person)\n",
        "\n",
        "\n",
        "\n",
        "#pydantic\n",
        "\n",
        "from pydantic import BaseModel, EmailStr, Field\n",
        "from typing import Optional\n",
        "\n",
        "class Student(BaseModel):\n",
        "\n",
        "    name: str = 'nitish'\n",
        "    age: Optional[int] = None\n",
        "    email: EmailStr\n",
        "    cgpa: float = Field(gt=0, lt=10, default=5, description='A decimal value representing the cgpa of the student')\n",
        "\n",
        "\n",
        "new_student = {'age':'32', 'email':'abc@gmail.com'}\n",
        "\n",
        "student = Student(**new_student)\n",
        "\n",
        "student_dict = dict(student)\n",
        "\n",
        "print(student_dict['age'])\n",
        "\n",
        "student_json = student.model_dump_json()\n",
        "\n",
        "\n",
        "\n",
        "#json schema\n",
        "\n",
        "{\n",
        "    \"title\": \"student\",\n",
        "    \"description\": \"schema about students\",\n",
        "    \"type\": \"object\",\n",
        "    \"properties\":{\n",
        "        \"name\":\"string\",\n",
        "        \"age\":\"integer\"\n",
        "    },\n",
        "    \"required\":[\"name\"]\n",
        "}"
      ],
      "metadata": {
        "id": "WvkhAMIrrqWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# typedict working example\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import TypedDict, Annotated, Optional, Literal\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# schema\n",
        "class Review(TypedDict):\n",
        "\n",
        "    key_themes: Annotated[list[str], \"Write down all the key themes discussed in the review in a list\"]\n",
        "    summary: Annotated[str, \"A brief summary of the review\"]\n",
        "    sentiment: Annotated[Literal[\"pos\", \"neg\"], \"Return sentiment of the review either negative, positive or neutral\"]\n",
        "    pros: Annotated[Optional[list[str]], \"Write down all the pros inside a list\"]\n",
        "    cons: Annotated[Optional[list[str]], \"Write down all the cons inside a list\"]\n",
        "    name: Annotated[Optional[str], \"Write the name of the reviewer\"]\n",
        "\n",
        "\n",
        "structured_model = model.with_structured_output(Review)\n",
        "\n",
        "result = structured_model.invoke(\"\"\"I recently upgraded to the Samsung Galaxy S24 Ultra, and I must say, it’s an absolute powerhouse! The Snapdragon 8 Gen 3 processor makes everything lightning fast—whether I’m gaming, multitasking, or editing photos. The 5000mAh battery easily lasts a full day even with heavy use, and the 45W fast charging is a lifesaver.\n",
        "\n",
        "The S-Pen integration is a great touch for note-taking and quick sketches, though I don't use it often. What really blew me away is the 200MP camera—the night mode is stunning, capturing crisp, vibrant images even in low light. Zooming up to 100x actually works well for distant objects, but anything beyond 30x loses quality.\n",
        "\n",
        "However, the weight and size make it a bit uncomfortable for one-handed use. Also, Samsung’s One UI still comes with bloatware—why do I need five different Samsung apps for things Google already provides? The $1,300 price tag is also a hard pill to swallow.\n",
        "\n",
        "Pros:\n",
        "Insanely powerful processor (great for gaming and productivity)\n",
        "Stunning 200MP camera with incredible zoom capabilities\n",
        "Long battery life with fast charging\n",
        "S-Pen support is unique and useful\n",
        "\n",
        "Review by Nitish Singh\n",
        "\"\"\")\n",
        "\n",
        "print(result['name'])"
      ],
      "metadata": {
        "id": "JEMqY0oovoqj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pydantic working example\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import TypedDict, Annotated, Optional, Literal\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# schema\n",
        "class Review(BaseModel):\n",
        "\n",
        "    key_themes: list[str] = Field(description=\"Write down all the key themes discussed in the review in a list\")\n",
        "    summary: str = Field(description=\"A brief summary of the review\")\n",
        "    sentiment: Literal[\"pos\", \"neg\"] = Field(description=\"Return sentiment of the review either negative, positive or neutral\")\n",
        "    pros: Optional[list[str]] = Field(default=None, description=\"Write down all the pros inside a list\")\n",
        "    cons: Optional[list[str]] = Field(default=None, description=\"Write down all the cons inside a list\")\n",
        "    name: Optional[str] = Field(default=None, description=\"Write the name of the reviewer\")\n",
        "\n",
        "\n",
        "structured_model = model.with_structured_output(Review)\n",
        "\n",
        "result = structured_model.invoke(\"\"\"I recently upgraded to the Samsung Galaxy S24 Ultra, and I must say, it’s an absolute powerhouse! The Snapdragon 8 Gen 3 processor makes everything lightning fast—whether I’m gaming, multitasking, or editing photos. The 5000mAh battery easily lasts a full day even with heavy use, and the 45W fast charging is a lifesaver.\n",
        "\n",
        "The S-Pen integration is a great touch for note-taking and quick sketches, though I don't use it often. What really blew me away is the 200MP camera—the night mode is stunning, capturing crisp, vibrant images even in low light. Zooming up to 100x actually works well for distant objects, but anything beyond 30x loses quality.\n",
        "\n",
        "However, the weight and size make it a bit uncomfortable for one-handed use. Also, Samsung’s One UI still comes with bloatware—why do I need five different Samsung apps for things Google already provides? The $1,300 price tag is also a hard pill to swallow.\n",
        "\n",
        "Pros:\n",
        "Insanely powerful processor (great for gaming and productivity)\n",
        "Stunning 200MP camera with incredible zoom capabilities\n",
        "Long battery life with fast charging\n",
        "S-Pen support is unique and useful\n",
        "\n",
        "Review by Nitish Singh\n",
        "\"\"\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "rb8Bj2YEv51P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# json schema working example\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from dotenv import load_dotenv\n",
        "from typing import TypedDict, Annotated, Optional, Literal\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "model = ChatOpenAI()\n",
        "\n",
        "# schema\n",
        "json_schema = {\n",
        "  \"title\": \"Review\",\n",
        "  \"type\": \"object\",\n",
        "  \"properties\": {\n",
        "    \"key_themes\": {\n",
        "      \"type\": \"array\",\n",
        "      \"items\": {\n",
        "        \"type\": \"string\"\n",
        "      },\n",
        "      \"description\": \"Write down all the key themes discussed in the review in a list\"\n",
        "    },\n",
        "    \"summary\": {\n",
        "      \"type\": \"string\",\n",
        "      \"description\": \"A brief summary of the review\"\n",
        "    },\n",
        "    \"sentiment\": {\n",
        "      \"type\": \"string\",\n",
        "      \"enum\": [\"pos\", \"neg\"],\n",
        "      \"description\": \"Return sentiment of the review either negative, positive or neutral\"\n",
        "    },\n",
        "    \"pros\": {\n",
        "      \"type\": [\"array\", \"null\"],\n",
        "      \"items\": {\n",
        "        \"type\": \"string\"\n",
        "      },\n",
        "      \"description\": \"Write down all the pros inside a list\"\n",
        "    },\n",
        "    \"cons\": {\n",
        "      \"type\": [\"array\", \"null\"],\n",
        "      \"items\": {\n",
        "        \"type\": \"string\"\n",
        "      },\n",
        "      \"description\": \"Write down all the cons inside a list\"\n",
        "    },\n",
        "    \"name\": {\n",
        "      \"type\": [\"string\", \"null\"],\n",
        "      \"description\": \"Write the name of the reviewer\"\n",
        "    }\n",
        "  },\n",
        "  \"required\": [\"key_themes\", \"summary\", \"sentiment\"]\n",
        "}\n",
        "\n",
        "\n",
        "structured_model = model.with_structured_output(json_schema)\n",
        "\n",
        "result = structured_model.invoke(\"\"\"I recently upgraded to the Samsung Galaxy S24 Ultra, and I must say, it’s an absolute powerhouse! The Snapdragon 8 Gen 3 processor makes everything lightning fast—whether I’m gaming, multitasking, or editing photos. The 5000mAh battery easily lasts a full day even with heavy use, and the 45W fast charging is a lifesaver.\n",
        "\n",
        "The S-Pen integration is a great touch for note-taking and quick sketches, though I don't use it often. What really blew me away is the 200MP camera—the night mode is stunning, capturing crisp, vibrant images even in low light. Zooming up to 100x actually works well for distant objects, but anything beyond 30x loses quality.\n",
        "\n",
        "However, the weight and size make it a bit uncomfortable for one-handed use. Also, Samsung’s One UI still comes with bloatware—why do I need five different Samsung apps for things Google already provides? The $1,300 price tag is also a hard pill to swallow.\n",
        "\n",
        "Pros:\n",
        "Insanely powerful processor (great for gaming and productivity)\n",
        "Stunning 200MP camera with incredible zoom capabilities\n",
        "Long battery life with fast charging\n",
        "S-Pen support is unique and useful\n",
        "\n",
        "Review by Nitish Singh\n",
        "\"\"\")\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "WZpw6wgAwZt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When These Methods Fail — Open Source Limitations**\n",
        "\n",
        "Structured output methods **rely on API-level support** from model providers.\n",
        "But open-source models (like TinyLLaMA, Mistral, Llama2 etc.) **don’t have native support** for:\n",
        "\n",
        "* Function calling\n",
        "* Native JSON enforcement\n",
        "* Tool use APIs\n",
        "\n",
        "So instead, we need to:\n",
        "\n",
        "```\n",
        "LLM Output (Free-form Text)\n",
        "        ↓\n",
        "Parse it with Output Parsers (covered below)\n",
        "```\n",
        "\n",
        "This is more fragile and error-prone, but **necessary** with local or open-source models.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "* If you're running `TinyLLaMA` on Hugging Face Transformers, it **can't enforce JSON or function calls**.\n",
        "* You’ll need to extract structured data from plain text using **Output Parsers** manually.\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "### **LangChain Output Parsers**\n",
        "\n",
        "**Output Parsers** in LangChain are utilities that help convert raw LLM outputs into structured formats such as JSON, Pydantic models, plain strings, or custom objects. They improve:\n",
        "\n",
        "* **Consistency**: The same structure every time\n",
        "* **Validation**: Errors are caught if the output doesn't match expected format\n",
        "* **Usability**: Easier downstream processing\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of Output Parsers**\n",
        "\n",
        "```\n",
        "LangChain Output Parsers\n",
        "_____________________________\n",
        "|                          |\n",
        "Simple                    Structured\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. StrOutputParser**\n",
        "\n",
        "* **Purpose**: Returns the LLM output as a plain string (no formatting or structure).\n",
        "* **Use case**: When you just want the raw response without parsing.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. JsonOutputParser**\n",
        "\n",
        "* **Purpose**: Parses model output as JSON using Python's `json.loads()`.\n",
        "* **Use case**: When the LLM is expected to return a well-formatted JSON string.\n",
        "* **Note**: Fragile if the model returns invalid JSON (missing quotes, commas, etc).\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. StructuredOutputParser**\n",
        "\n",
        "* **Purpose**: Extracts structured JSON from LLM output based on **ResponseSchema**.\n",
        "* **How it works**:\n",
        "  You define expected fields with types using `ResponseSchema`, and it guides the model output format.\n",
        "* **Example use case**:\n",
        "  Extracting fields like `{\"title\": ..., \"date\": ..., \"summary\": ...}` reliably from the output.\n",
        "\n",
        "##### 🔹 How is this different from model-native structured output?\n",
        "\n",
        "* Previously (with `Pydantic`, `TypedDict`, or `JSON Schema`), structure was **enforced at generation time**.\n",
        "* With `StructuredOutputParser`, structure is **parsed after generation**, based on expected schema.\n",
        "* This method works **even with models that don’t support function calling or native JSON output**.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. PydanticOutputParser**\n",
        "\n",
        "#### **What is it?**\n",
        "\n",
        "* A parser that uses **Pydantic models** to define and validate the output schema.\n",
        "\n",
        "#### **Why use it?**\n",
        "\n",
        "* **Strict Schema Enforcement**: Guarantees output matches the schema.\n",
        "* **Type Safety**: Converts JSON or text into Python `BaseModel` objects.\n",
        "* **Error Handling**: Easily identify missing or invalid fields.\n",
        "* **Validation Logic**: Leverage all features of Pydantic (e.g., field constraints, validators).\n",
        "* **Integration**: Works smoothly with LangChain prompts and chains.\n",
        "\n",
        "#### **When to use:**\n",
        "\n",
        "* When your output needs to be **strongly typed**, **validated**, and easily **converted into Python objects**.\n",
        "* Ideal for structured use cases like form filling, database entry, or configuration generation.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f2RQ8NpmyprG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output parsers\n",
        "\n"
      ],
      "metadata": {
        "id": "dswpn2PK0pSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains\n"
      ],
      "metadata": {
        "id": "13VV3aej_g9Q"
      }
    }
  ]
}